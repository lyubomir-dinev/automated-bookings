# Automated Room/Desk Booking

### Table of contents
1. [Background and requirements](#1-background-and-requirements)

    1.1. [Business requirements](#11-business-requirements)

    1.2. [Internal requirements](#12-internal-requirements)

    1.3. [Available services and models](#13-available-services-and-models)

2. [Exploration and estimates](#2-exploration-and-estimates)

    2.1. [Front end manipulation of the data](#21-front-end-manipulation-of-the-data)

    2.1.1. [Custom UX](#211-custom-ux)

    2.1.2. [Procedural approach](#212-procedural-approach)

    2.2. [Data generation and storage](#22-data-generation-and-storage)

    2.3. [Estimates](#23-estimates)

3. [Implementation](#3-implementation)
    
    3.1. [BE - Masking the input image](#31-be---masking-the-input-image)

    3.2. [BE - Contour processing](#32-be---contour-processing)


### 1. Background and requirements

#### 1.1. Business requirements
Clients wanted a system where they can manage and monitor the bookings of desks and rooms. The system needed to support the following:

1. Customization of amenities per room and desk
2. Permanent bookings of desks 
3. Long term bookings of rooms and desks
4. Quick and easy updating of the overal layout

#### 1.2. Internal requirements
The system should not introduce additional maintenance burden to any team within the company (design, support, client relations, engineering, etc.).

The system should be intelligent enough to suggest the majority of metadata captured and stored against a book and desk (location, size, name, etc.)

#### 1.3. Available services and models
There was an existing rudimentary service for handling room bookings. The model essentially consisted of 2 tables (**rooms** and **bookings**)

##### Rooms
Column | Type 
--- | ---
roomId | varchar (uuid) - **PK**
roomName | varchar
capacity | smallint
locationId | varchar (uuid) - _FK_
notes | text

##### Bookings
Column | Type
--- | ---
roomId | varchar (uuid) - _FK_
bookingId | varchar (uuid) - **PK**
userId | varchar (uuid) - _FK_
bookingStart | timestamp
bookingEnd | timestamp

---

### 2. Exploration and estimates

The problem was split into two separate domains:
1. Front end manipulation of the data 
2. Data generation and storage 

#### 2.1. Front end manipulation of the data
Two approaches were explored: 
1. Building a custom UX to handle the drawing and configuration of the rooms and desks within the office layout
2. Using a procedural approach relaying on the data generated by the back end

##### 2.1.1. Custom UX
Other parts of the system had similar UX available and were producing positive results and feedback from the clients. However, reusing their code would have introduced too much refactoring and the final UX would have introduced too much complexity and maintenance burden to the teams responcible for the maintenance of the system.

___As such, this approach was discarded!.___

##### 2.1.2. Procedural approach
The idea was to generate data describing the rooms, desks and overal office layout based on a visual input provided by the design team in the form of a PNG image of an office layout schematic. The image would be read by OpenCV and the relevant artefacts would be identified and extracted as a collection of objects containing vertices, dimensions and other metadata.

This data would then be plugged be used to generate an SVG on the FE allowing for event handlers to be assigned against the elements of interest, thus adding interractivity to the layout. Although this approach meant more more ground work was needed and a robust be to be created, it was the most flexible and maintainable solution.

#### 2.2. Data generation and storage
OpenCV was the chosen technology that would process the input image and generate the tokenized output. A decision was made to use Python instead of C++ as the language of choice for the back end as it there was bettern knowledge within the wider team. 

As there was an existing service for handling room bookings, the decision was made to refactor and addapt it to the new model and to write the Python code as a stand alone script to be called by the existing Golang service framework.

The Python script would produce an output similar to the following format:

```JSON
{
	"walls": [
		{
			"id": "392745fd-c7a3-4ebd-88b4-ed6fc7c7c346",
			"center": { "x": 5036.4609375, "y": 3203.640380859375 },
			"angle": 0,
			"vertices": [
				{ "x": 4818, "y": 2667 },
				{ "x": 5257, "y": 2667 },
				...
			]
		},
		...
	],
	"desks": [
		{
			"id": "392745fd-c7a3-4ebd-88b4-ed6fc7c7c346",
			"center": { "x": 5871.74853515625, "y": 4129.25146484375 },
			"angle": 45,
			"vertices": [
				{ "x": 5849, "y": 4020 },
				{ "x": 5850, "y": 4020 },
				...
			]
		},
		...
	],
	"collaboration_areas": [],
	"meeting_rooms": [
		{
			"id": "392745fd-c7a3-4ebd-88b4-ed6fc7c7c346",
			"center": { "x": 6239.53125, "y": 4774.79443359375 },
			"angle": 45,
			"vertices": [
				{ "x": 6122, "y": 4298 },
				{ "x": 6123, "y": 4298 },
				...
			]
		},
		...
	],
	"zones_of_interest": [
		{
			"id": "392745fd-c7a3-4ebd-88b4-ed6fc7c7c346",
			"center": { "x": 4540.28955078125, "y": 5284.7392578125 },
			"angle": 0,
			"vertices": [
				{ "x": 4429, "y": 5026 },
				{ "x": 4431, "y": 5026 },
				...
			]
		},
		...
	],
}
```

The JSON output would be consumed by the Golang service and mapped against the DB models. The refectored db models would look as follows:

##### Features
Column | Type
--- | ---
featureId | varchar (uuid) - **PK**
featureType | smallint
angle | smallint
center | text
vertices | text

__Notes:__ Although Postgress (the db engine we used) has native support for storing and querying JSON data, it was decided to store the data as text as the data was not going to be queried and the overhead of parsing the JSON would have been too high. Furthermore, although Postgress has native support for Enums as well, the featureType column was defined as smalling storing an Enum maintained in the Golang service, elevating the complexity from the db engine to the service.

##### Rooms
Column | Type
--- | ---
roomId | varchar (uuid) - **PK**
roomName | varchar
capacity | smallint
locationId | varchar (uuid) - _FK_
notes | text
featureId | varchar (uuid) - _FK_

##### Bookings
Column | Type
--- | ---
bookingId | varchar (uuid) - **PK**
entityId | varchar (uuid) - _FK_
featureType | smallint
userId | varchar (uuid) - _FK_
bookingStart | timestamp
bookingEnd | timestamp

##### Desks
Column | Type
--- | ---
deskId | varchar (uuid) - **PK**
featureId | varchar (uuid) - _FK_
locationId | varchar (uuid) - _FK_

##### Amenities
Column | Type
--- | ---
amenityId | varchar (uuid) - **PK**
amenityTypeId | varchar (uuid) - _FK_
entityId | varchar (uuid) - _FK_

##### Amenities Types
Column | Type
--- | ---
amenityTypeId | varchar (uuid) - **PK**
amenityTypeName | varchar
featureType | smallint

__Notes:__ The Amenities and Amenities Types tables were introduced to allow for the customization of the amenities per room and desk. The Amenities Types table would be populated with the default amenities and the Amenities table would be populated with the amenities per room and desk. The Amenities Types table would be used to populate the UI with the available amenities  allowing for searching/filtering and the Amenities table would be used to populate the UI with the amenities per room and desk.

#### 2.3. Estimates
The initial estimate suggested I would need 1 month to explore and understand OpenCV and produce a working prototype and 3 months to produce an MVP. 

---

### 3. Implementation

#### 3.1. BE - Masking the input image
The first step was to mask the input image to allow for easier processing and object type identification. The masking was done using the following steps:

1. In order for the script to identify geometry within the image, the following colors need to be used to identify the shapes:

Zone of Interest | Color | RGB Code | Hex Code
--- | --- | --- | ---
Bounding Walls | Red | rgb(255,0,0) | #FF0000
Banks of Desks | Blue | rgb(0,0,255) | #0000FF
Collaboration Areas | Green | rgb(0,255,0)| #00FF00
Meeting Rooms | Orange | rgb(255,165,0)| #FFA500
Zones of Interest | Purple | rgb(128,0,128) | #800080

2. The following function was used to generate masks for each of the colors:

```python
def generateColorMask(base_bgr, frame):

    lower_range = np.array([max(0, base_bgr[i] - constants.COLOR_MASK_LEEWAY) for i in range(3)])
    upper_range = np.array([min(255, base_bgr[i] + constants.COLOR_MASK_LEEWAY) for i in range(3)])

    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
	# Create a mask for the desired color range
    mask = cv2.inRange(frame_bgr, lower_range, upper_range)

    mask = mask.astype(np.uint8)*255

    return mask
```

__Notes:__ the ```constants.COLOR_MAST_LEEWAY``` was set to 50 to allow for some leeway in the color range. This was needed as the colors in the image were not absolutely precise and the leeway allowed for better identification of the shapes.

3. The masked image was then processed by the following script which identified any contours of interest, parsed out any artefacts such as contours that were generated by the algorithm itself, or contours that didn't meet the base critera for length: 

```python
    contours, hierarchy = cv2.findContours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)

    # get only contours with hierarchy 1
    contours = [contours[i] for i in range(len(contours)) if hierarchy[0][i][3] != -1]
    
    # get only contours with more than 5 points
    contours = [contour for contour in contours if len(contour) > 5]
```

__Notes:__ the function ```findContours``` accepts a mode parameter which dictates to the function how to organize the identified contours. In our use case the best mode was the ```RETR_CCOMP``` which organizes the contours in a 2 level hierarchy. The first level contains the outer contours of the shapes and the second level contains the contours of the holes within the shapes. The ```CHAIN_APPROX_NONE``` parameter dictates the algorithm to return all the points of the contours allowing for complex shapes such as __curves__ and __complex polygons__.

#### 3.2. BE - Contour processing

Each contour was then processed and the relevant information was extracted/optimized: 

```python
    for idx, contour in enumerate(contours):
```

1. Starting with calculating a bounding shape - a __box__ if we're processing the desk contours or an __ellipse__ for everything else

```python
        (x,y),(w,h),angle = approximationType == "ellipse" and cv2.fitEllipseDirect(contour) or cv2.minAreaRect(contour)
        polygon = np.array([contour[:, 0, :]])
        [x1,y1,tw,th] = lir_within_polygon.largest_interior_rectangle(polygon)
        midX1 = x1 + tw/2
        midY1 = y1 + th/2
        angle = math.floor(angle)
```

2. With that information available we start constructing the actual contour payload object. For the rectangular contours (__desks__) we also add the __width__ and __height__ of the contour. This helps with the drawing of SVG boxes, saving on the calculation on the FE. 

```python 
        contours[idx] = {"id": str(uuid.uuid4()), "angle": int(angle), "center": {"x": int(x), "y": int(y)}, "innerRect": {"width": int(tw), "height": int(th), "x": int(midX1), "y": int(midY1)}}
        if(approximationType=="rect"):
            contours[idx]["width"] = int(w)
            contours[idx]["height"] = int(h)
```

3. The script would support 2 different types of ```vertice``` generation. The __optimised__ approach is supported only for layouts that contain only straight lines. The intention is to reduce as much as possible the number of generated vertices and as a result of that reducing the stored data, transferred data between the BE and FE, number of vertices rendered on the FE and ultimately memory used by the application. The __non-optimised__ approach is used for layouts that contain curves and complex polygons. In those cases, approximating a good number of vertices is not fesible for the BE. 

    The key function used as part of the __optimised__ approach is ```goodFeaturesToTrack```. It accepts 4 main parameters:
    
    - ```image``` - the image to be processed
    - ```maxCorners``` - the maximum number of corners to be identified
    - ```qualityLevel``` - the minimum quality level of the corners to be identified
    - ```minDistance``` - the minimum distance between the identified corners
    
    Adjusting the latter 3 parameters allows for a good balance between the number of vertices and the accuracy of the contour. The ```goodFeaturesToTrack``` function returns a list of corners which are then sorted in a clockwise fashion.

```python
        if optimise_vertices:
            contour_mask = np.zeros(mask.shape, np.uint8)
            contour_mask = cv2.fillPoly(contour_mask, pts=[contour], color=(255, 255, 255))
            corners = cv2.goodFeaturesToTrack(image=contour_mask, maxCorners=25, qualityLevel=0.3, minDistance=15)
            contours[idx]["vertices"] = sortCorners(contour, [[int(c[0]), int(c[1])] for c in corners[:, 0, :]])
        else:
            contours[idx]["vertices"] = contour[:, 0, :]
```

4. The final step is to cache the top left vertice of the contour and calculate the area of the contour.

    - The ```getMinXY``` function was used to identify the top left corner of the contour of each contour, keep a global track of those coordinates for later use as a translation basis of all contour vertices in order to remove any unnecessary padding from the produced data set.
    - The ```area``` was calculated using the ```contourArea``` function which calculates the area of the contour using the Green's theorem and was used after all contourls have been identified to sort them in an ascending order for the largest having the smallest "z-index". This reduced the risk of incorrectly ordering the polygins when drawing them on the SVG in the UX.


```python 
        getMinXY(contours[idx]["vertices"])
        contours[idx]["area"] = int(cv2.contourArea(np.array(contours[idx]["vertices"])))
        contours[idx]["vertices"] = [{'x': int(sublist[0]), 'y': int(sublist[1])} for sublist in contours[idx]["vertices"]]
```